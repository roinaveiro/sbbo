{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b49ac645",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/roinaveiro/miniconda/envs/tf-sbbo/lib/python3.8/site-packages/gpflow/experimental/utils.py:42: UserWarning: You're calling gpflow.experimental.check_shapes.decorator.check_shapes which is considered *experimental*. Expect: breaking changes, poor documentation, and bugs.\n",
      "  warn(\n",
      "/Users/roinaveiro/miniconda/envs/tf-sbbo/lib/python3.8/site-packages/gpflow/experimental/utils.py:42: UserWarning: You're calling gpflow.experimental.check_shapes.inheritance.inherit_check_shapes which is considered *experimental*. Expect: breaking changes, poor documentation, and bugs.\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "# setting path\n",
    "sys.path.append('..')\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "from src.models.bocs.LinReg import LinReg\n",
    "from src.models.GPr import GPr\n",
    "from ngboost import NGBRegressor\n",
    "from ngboost.distns import LogNormal, Normal, Exponential\n",
    "from ngboost.scores import CRPS, LogScore\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Learners\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from src.problems.contamination import Contamination\n",
    "from src.problems.rna import RNA\n",
    "from src.problems.bqp import BQP\n",
    "from src.problems.latin_square import LatinSquare\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "\n",
    "from src.uncertainty_metrics import *\n",
    "\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a06bddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_empirical_coverage(opt, n_exp, test_set_size=0.2, nb=30):\n",
    "    \n",
    "    \n",
    "    names  = [\"GPr\", \"BOCS\", \"NGBlinCV\", \"NGBdec\"]\n",
    "        \n",
    "    X = opt.X\n",
    "    y = opt.y\n",
    "    \n",
    "    exp     = np.array([])\n",
    "    alg     = np.array([])\n",
    "    score_m = np.array([])\n",
    "    rmse_m  = np.array([])\n",
    "    mae_m   = np.array([])\n",
    "    q_m     = np.array([])\n",
    "    eq_m    = np.array([])\n",
    "    \n",
    "    for j in range(n_exp):\n",
    "        \n",
    "        print(\"Exp\", j)\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_set_size)\n",
    "        \n",
    "        m1 = GPr()\n",
    "        m2 = LinReg(nVars=25, order=2)\n",
    "        learner = LassoCV(cv=5)\n",
    "        m3 = NGBRegressor(Base=learner)\n",
    "        learner = DecisionTreeRegressor(criterion='friedman_mse', max_depth=5)\n",
    "        m4 = NGBRegressor(Base=learner)\n",
    "\n",
    "        models = [m1, m2, m3, m4]\n",
    "        \n",
    "        for i, m in enumerate(models):\n",
    "            score, rmse, mae, q, eq = get_results_model(m, X_train, y_train, X_test, y_test, nb)\n",
    "            \n",
    "            exp     = np.hstack((exp, j*np.ones(nb)))\n",
    "            alg     = np.hstack((alg, np.array(nb * [names[i]] )))\n",
    "            score_m = np.hstack((score_m, score*np.ones(nb)))\n",
    "            rmse_m  = np.hstack((rmse_m, rmse*np.ones(nb)))\n",
    "            mae_m   = np.hstack((mae_m, mae*np.ones(nb)))\n",
    "            q_m     = np.hstack((q_m, q))\n",
    "            eq_m    = np.hstack((eq_m, eq))\n",
    "            \n",
    "\n",
    "    df = pd.DataFrame({\"Experiment\" : exp,\n",
    "                       \"Algorithm\"  : alg,\n",
    "                       \"R2\"         : score_m,\n",
    "                       \"RMSE\"       : rmse_m,\n",
    "                       \"MAE\"        : mae_m,\n",
    "                       \"Quantile\"   : q_m,\n",
    "                       \"E-quantile\" : eq_m})\n",
    "    \n",
    "    return df\n",
    "        \n",
    "\n",
    "def get_results_model(m, X_train, y_train, X_test, y_test, nb=30):\n",
    "   \n",
    "    m.fit(X_train, y_train)\n",
    "    y_pred = m.predict(X_test)\n",
    "    y_pred_d = m.pred_dist(X_test)\n",
    "    \n",
    "    # Output R^2, RMSE and MAE on the test set\n",
    "    score = r2_score(y_test, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    \n",
    "    scorer = CoveragePlot()\n",
    "    q, eq = scorer.compute(y_test, y_pred_d, num_bins=nb)\n",
    "    \n",
    "    return score, rmse, mae, q, eq\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff7f02a",
   "metadata": {},
   "source": [
    "# Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff9ea34",
   "metadata": {},
   "source": [
    "# Contamination problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a35b8b95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exp 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/roinaveiro/miniconda/envs/tf-sbbo/lib/python3.8/site-packages/gpflow/experimental/utils.py:42: UserWarning: You're calling gpflow.experimental.check_shapes.checker.ShapeChecker.__init__ which is considered *experimental*. Expect: breaking changes, poor documentation, and bugs.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempt Gibbs 1\n",
      "Iter Gibbs 0\n",
      "Iter Gibbs 100\n",
      "Iter Gibbs 200\n",
      "Iter Gibbs 300\n",
      "Iter Gibbs 400\n",
      "Iter Gibbs 500\n",
      "Iter Gibbs 600\n",
      "Iter Gibbs 700\n",
      "Iter Gibbs 800\n",
      "Iter Gibbs 900\n",
      "Iter Gibbs 1000\n",
      "Iter Gibbs 1100\n",
      "Iter Gibbs 1200\n",
      "Iter Gibbs 1300\n",
      "Iter Gibbs 1400\n",
      "Iter Gibbs 1500\n",
      "Iter Gibbs 1600\n",
      "Iter Gibbs 1700\n",
      "Iter Gibbs 1800\n",
      "Iter Gibbs 1900\n",
      "[iter 0] loss=0.8798 val_loss=0.0000 scale=2.0000 norm=1.4328\n",
      "[iter 100] loss=0.0889 val_loss=0.0000 scale=2.0000 norm=0.9209\n",
      "[iter 200] loss=0.0113 val_loss=0.0000 scale=2.0000 norm=1.0451\n",
      "[iter 300] loss=0.0085 val_loss=0.0000 scale=2.0000 norm=1.0952\n",
      "[iter 400] loss=0.0085 val_loss=0.0000 scale=2.0000 norm=1.1030\n",
      "[iter 0] loss=0.8798 val_loss=0.0000 scale=2.0000 norm=1.4328\n",
      "[iter 100] loss=-0.3332 val_loss=0.0000 scale=2.0000 norm=0.9565\n",
      "[iter 200] loss=-1.3522 val_loss=0.0000 scale=4.0000 norm=1.9846\n",
      "[iter 300] loss=-3.3519 val_loss=0.0000 scale=4.0000 norm=1.9996\n",
      "[iter 400] loss=-6.6519 val_loss=0.0000 scale=8.0000 norm=4.0000\n",
      "Exp 1\n",
      "Attempt Gibbs 1\n",
      "Iter Gibbs 0\n",
      "Iter Gibbs 100\n",
      "Iter Gibbs 200\n",
      "Iter Gibbs 300\n",
      "Iter Gibbs 400\n",
      "Iter Gibbs 500\n",
      "Iter Gibbs 600\n",
      "Iter Gibbs 700\n",
      "Iter Gibbs 800\n",
      "Iter Gibbs 900\n",
      "Iter Gibbs 1000\n",
      "Iter Gibbs 1100\n",
      "Iter Gibbs 1200\n",
      "Iter Gibbs 1300\n",
      "Iter Gibbs 1400\n",
      "Iter Gibbs 1500\n",
      "Iter Gibbs 1600\n",
      "Iter Gibbs 1700\n",
      "Iter Gibbs 1800\n",
      "Iter Gibbs 1900\n",
      "[iter 0] loss=0.9328 val_loss=0.0000 scale=2.0000 norm=1.4582\n",
      "[iter 100] loss=0.4668 val_loss=0.0000 scale=2.0000 norm=1.1175\n",
      "[iter 200] loss=0.4532 val_loss=0.0000 scale=2.0000 norm=1.1850\n",
      "[iter 300] loss=0.4528 val_loss=0.0000 scale=2.0000 norm=1.1986\n",
      "[iter 400] loss=0.4528 val_loss=0.0000 scale=2.0000 norm=1.2005\n",
      "== Quitting at iteration / GRAD 459\n",
      "[iter 0] loss=0.9328 val_loss=0.0000 scale=2.0000 norm=1.4582\n",
      "[iter 100] loss=-0.2774 val_loss=0.0000 scale=2.0000 norm=0.9588\n",
      "[iter 200] loss=-1.3170 val_loss=0.0000 scale=4.0000 norm=1.9857\n",
      "[iter 300] loss=-3.3168 val_loss=0.0000 scale=4.0000 norm=1.9997\n",
      "[iter 400] loss=-6.7968 val_loss=0.0000 scale=8.0000 norm=4.0000\n",
      "Exp 2\n",
      "Attempt Gibbs 1\n",
      "Iter Gibbs 0\n",
      "Iter Gibbs 100\n",
      "Iter Gibbs 200\n",
      "Iter Gibbs 300\n",
      "Iter Gibbs 400\n",
      "Iter Gibbs 500\n",
      "Iter Gibbs 600\n",
      "Iter Gibbs 700\n",
      "Iter Gibbs 800\n",
      "Iter Gibbs 900\n",
      "Iter Gibbs 1000\n",
      "Iter Gibbs 1100\n",
      "Iter Gibbs 1200\n",
      "Iter Gibbs 1300\n",
      "Iter Gibbs 1400\n",
      "Iter Gibbs 1500\n",
      "Iter Gibbs 1600\n",
      "Iter Gibbs 1700\n",
      "Iter Gibbs 1800\n",
      "Iter Gibbs 1900\n",
      "[iter 0] loss=0.8753 val_loss=0.0000 scale=2.0000 norm=1.4206\n",
      "[iter 100] loss=0.3035 val_loss=0.0000 scale=2.0000 norm=1.0286\n",
      "[iter 200] loss=0.2823 val_loss=0.0000 scale=2.0000 norm=1.1111\n",
      "[iter 300] loss=0.2817 val_loss=0.0000 scale=2.0000 norm=1.1296\n",
      "[iter 400] loss=0.2817 val_loss=0.0000 scale=2.0000 norm=1.1322\n",
      "== Quitting at iteration / GRAD 472\n",
      "[iter 0] loss=0.8753 val_loss=0.0000 scale=2.0000 norm=1.4206\n",
      "[iter 100] loss=-0.3382 val_loss=0.0000 scale=2.0000 norm=0.9576\n",
      "[iter 200] loss=-1.3879 val_loss=0.0000 scale=4.0000 norm=1.9860\n",
      "[iter 300] loss=-3.3876 val_loss=0.0000 scale=4.0000 norm=1.9997\n",
      "[iter 400] loss=-6.8476 val_loss=0.0000 scale=8.0000 norm=4.0000\n",
      "Exp 3\n",
      "Attempt Gibbs 1\n",
      "Iter Gibbs 0\n",
      "Iter Gibbs 100\n",
      "Iter Gibbs 200\n",
      "Iter Gibbs 300\n",
      "Iter Gibbs 400\n",
      "Iter Gibbs 500\n",
      "Iter Gibbs 600\n",
      "Iter Gibbs 700\n",
      "Iter Gibbs 800\n",
      "Iter Gibbs 900\n",
      "Iter Gibbs 1000\n",
      "Iter Gibbs 1100\n",
      "Iter Gibbs 1200\n",
      "Iter Gibbs 1300\n",
      "Iter Gibbs 1400\n",
      "Iter Gibbs 1500\n",
      "Iter Gibbs 1600\n",
      "Iter Gibbs 1700\n",
      "Iter Gibbs 1800\n",
      "Iter Gibbs 1900\n",
      "[iter 0] loss=0.8943 val_loss=0.0000 scale=2.0000 norm=1.4269\n",
      "[iter 100] loss=0.3915 val_loss=0.0000 scale=2.0000 norm=1.0521\n",
      "[iter 200] loss=0.3749 val_loss=0.0000 scale=2.0000 norm=1.1081\n",
      "[iter 300] loss=0.3745 val_loss=0.0000 scale=2.0000 norm=1.1211\n",
      "[iter 400] loss=0.3745 val_loss=0.0000 scale=2.0000 norm=1.1230\n",
      "== Quitting at iteration / GRAD 465\n",
      "[iter 0] loss=0.8943 val_loss=0.0000 scale=2.0000 norm=1.4269\n",
      "[iter 100] loss=-0.3212 val_loss=0.0000 scale=2.0000 norm=0.9598\n",
      "[iter 200] loss=-1.3912 val_loss=0.0000 scale=4.0000 norm=1.9875\n",
      "[iter 300] loss=-3.3909 val_loss=0.0000 scale=4.0000 norm=1.9997\n",
      "[iter 400] loss=-6.8709 val_loss=0.0000 scale=8.0000 norm=4.0000\n",
      "Exp 4\n",
      "Attempt Gibbs 1\n",
      "Iter Gibbs 0\n",
      "Iter Gibbs 100\n",
      "Iter Gibbs 200\n",
      "Iter Gibbs 300\n",
      "Iter Gibbs 400\n",
      "Iter Gibbs 500\n",
      "Iter Gibbs 600\n",
      "Iter Gibbs 700\n",
      "Iter Gibbs 800\n",
      "Iter Gibbs 900\n",
      "Iter Gibbs 1000\n",
      "Iter Gibbs 1100\n",
      "Iter Gibbs 1200\n",
      "Iter Gibbs 1300\n",
      "Iter Gibbs 1400\n",
      "Iter Gibbs 1500\n",
      "Iter Gibbs 1600\n",
      "Iter Gibbs 1700\n",
      "Iter Gibbs 1800\n",
      "Iter Gibbs 1900\n",
      "[iter 0] loss=0.9315 val_loss=0.0000 scale=2.0000 norm=1.4527\n",
      "[iter 100] loss=0.0990 val_loss=0.0000 scale=2.0000 norm=0.8908\n",
      "[iter 200] loss=-0.0190 val_loss=0.0000 scale=2.0000 norm=0.9746\n",
      "[iter 300] loss=-0.0243 val_loss=0.0000 scale=2.0000 norm=1.0261\n",
      "[iter 400] loss=-0.0244 val_loss=0.0000 scale=2.0000 norm=1.0347\n",
      "[iter 0] loss=0.9315 val_loss=0.0000 scale=2.0000 norm=1.4527\n",
      "[iter 100] loss=-0.2659 val_loss=0.0000 scale=2.0000 norm=0.9564\n",
      "[iter 200] loss=-1.2648 val_loss=0.0000 scale=2.0000 norm=0.9913\n",
      "[iter 300] loss=-3.2446 val_loss=0.0000 scale=4.0000 norm=1.9996\n",
      "[iter 400] loss=-6.5846 val_loss=0.0000 scale=8.0000 norm=4.0000\n",
      "Exp 5\n",
      "Attempt Gibbs 1\n",
      "Iter Gibbs 0\n",
      "Iter Gibbs 100\n",
      "Iter Gibbs 200\n",
      "Iter Gibbs 300\n",
      "Iter Gibbs 400\n",
      "Iter Gibbs 500\n",
      "Iter Gibbs 600\n",
      "Iter Gibbs 700\n",
      "Iter Gibbs 800\n",
      "Iter Gibbs 900\n",
      "Iter Gibbs 1000\n",
      "Iter Gibbs 1100\n",
      "Iter Gibbs 1200\n",
      "Iter Gibbs 1300\n",
      "Iter Gibbs 1400\n",
      "Iter Gibbs 1500\n",
      "Iter Gibbs 1600\n",
      "Iter Gibbs 1700\n",
      "Iter Gibbs 1800\n",
      "Iter Gibbs 1900\n",
      "[iter 0] loss=0.9603 val_loss=0.0000 scale=2.0000 norm=1.4717\n",
      "[iter 100] loss=0.4706 val_loss=0.0000 scale=2.0000 norm=1.0808\n",
      "[iter 200] loss=0.4522 val_loss=0.0000 scale=2.0000 norm=1.1581\n",
      "[iter 300] loss=0.4517 val_loss=0.0000 scale=2.0000 norm=1.1750\n",
      "[iter 400] loss=0.4517 val_loss=0.0000 scale=2.0000 norm=1.1774\n",
      "== Quitting at iteration / GRAD 468\n",
      "[iter 0] loss=0.9603 val_loss=0.0000 scale=2.0000 norm=1.4717\n",
      "[iter 100] loss=-0.2515 val_loss=0.0000 scale=2.0000 norm=0.9625\n",
      "[iter 200] loss=-1.3619 val_loss=0.0000 scale=4.0000 norm=1.9887\n",
      "[iter 300] loss=-3.3617 val_loss=0.0000 scale=4.0000 norm=1.9997\n",
      "[iter 400] loss=-6.8417 val_loss=0.0000 scale=8.0000 norm=4.0000\n",
      "Exp 6\n",
      "Attempt Gibbs 1\n",
      "Iter Gibbs 0\n",
      "Iter Gibbs 100\n",
      "Iter Gibbs 200\n",
      "Iter Gibbs 300\n",
      "Iter Gibbs 400\n",
      "Iter Gibbs 500\n",
      "Iter Gibbs 600\n",
      "Iter Gibbs 700\n",
      "Iter Gibbs 800\n",
      "Iter Gibbs 900\n",
      "Iter Gibbs 1000\n",
      "Iter Gibbs 1100\n",
      "Iter Gibbs 1200\n",
      "Iter Gibbs 1300\n",
      "Iter Gibbs 1400\n",
      "Iter Gibbs 1500\n",
      "Iter Gibbs 1600\n",
      "Iter Gibbs 1700\n",
      "Iter Gibbs 1800\n",
      "Iter Gibbs 1900\n",
      "[iter 0] loss=0.9020 val_loss=0.0000 scale=2.0000 norm=1.4197\n",
      "[iter 100] loss=0.1870 val_loss=0.0000 scale=2.0000 norm=0.8916\n",
      "[iter 200] loss=0.1243 val_loss=0.0000 scale=2.0000 norm=0.9530\n",
      "[iter 300] loss=0.1223 val_loss=0.0000 scale=2.0000 norm=0.9839\n",
      "[iter 400] loss=0.1222 val_loss=0.0000 scale=2.0000 norm=0.9889\n",
      "[iter 0] loss=0.9020 val_loss=0.0000 scale=2.0000 norm=1.4197\n",
      "[iter 100] loss=-0.3102 val_loss=0.0000 scale=2.0000 norm=0.9605\n",
      "[iter 200] loss=-1.4109 val_loss=0.0000 scale=4.0000 norm=1.9892\n",
      "[iter 300] loss=-3.4108 val_loss=0.0000 scale=4.0000 norm=1.9998\n",
      "[iter 400] loss=-7.0508 val_loss=0.0000 scale=8.0000 norm=4.0000\n",
      "Exp 7\n",
      "Attempt Gibbs 1\n",
      "Iter Gibbs 0\n",
      "Iter Gibbs 100\n",
      "Iter Gibbs 200\n",
      "Iter Gibbs 300\n",
      "Iter Gibbs 400\n",
      "Iter Gibbs 500\n",
      "Iter Gibbs 600\n",
      "Iter Gibbs 700\n",
      "Iter Gibbs 800\n",
      "Iter Gibbs 900\n",
      "Iter Gibbs 1000\n",
      "Iter Gibbs 1100\n",
      "Iter Gibbs 1200\n",
      "Iter Gibbs 1300\n",
      "Iter Gibbs 1400\n",
      "Iter Gibbs 1500\n",
      "Iter Gibbs 1600\n",
      "Iter Gibbs 1700\n",
      "Iter Gibbs 1800\n",
      "Iter Gibbs 1900\n",
      "[iter 0] loss=0.9325 val_loss=0.0000 scale=2.0000 norm=1.4462\n",
      "[iter 100] loss=0.4028 val_loss=0.0000 scale=2.0000 norm=1.0210\n",
      "[iter 200] loss=0.3784 val_loss=0.0000 scale=2.0000 norm=1.0936\n",
      "[iter 300] loss=0.3777 val_loss=0.0000 scale=2.0000 norm=1.1116\n",
      "[iter 400] loss=0.3777 val_loss=0.0000 scale=2.0000 norm=1.1143\n",
      "== Quitting at iteration / GRAD 477\n",
      "[iter 0] loss=0.9325 val_loss=0.0000 scale=2.0000 norm=1.4462\n",
      "[iter 100] loss=-0.2718 val_loss=0.0000 scale=2.0000 norm=0.9571\n",
      "[iter 200] loss=-1.3123 val_loss=0.0000 scale=4.0000 norm=1.9863\n",
      "[iter 300] loss=-3.3121 val_loss=0.0000 scale=4.0000 norm=1.9997\n",
      "[iter 400] loss=-6.7121 val_loss=0.0000 scale=8.0000 norm=4.0000\n",
      "Exp 8\n",
      "Attempt Gibbs 1\n",
      "Iter Gibbs 0\n",
      "Iter Gibbs 100\n",
      "Iter Gibbs 200\n",
      "Iter Gibbs 300\n",
      "Iter Gibbs 400\n",
      "Iter Gibbs 500\n",
      "Iter Gibbs 600\n",
      "Iter Gibbs 700\n",
      "Iter Gibbs 800\n",
      "Iter Gibbs 900\n",
      "Iter Gibbs 1000\n",
      "Iter Gibbs 1100\n",
      "Iter Gibbs 1200\n",
      "Iter Gibbs 1300\n",
      "Iter Gibbs 1400\n",
      "Iter Gibbs 1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter Gibbs 1600\n",
      "Iter Gibbs 1700\n",
      "Iter Gibbs 1800\n",
      "Iter Gibbs 1900\n",
      "[iter 0] loss=0.9279 val_loss=0.0000 scale=2.0000 norm=1.4474\n",
      "[iter 100] loss=0.4030 val_loss=0.0000 scale=2.0000 norm=1.0342\n",
      "[iter 200] loss=0.3825 val_loss=0.0000 scale=2.0000 norm=1.1005\n",
      "[iter 300] loss=0.3820 val_loss=0.0000 scale=2.0000 norm=1.1165\n",
      "[iter 400] loss=0.3820 val_loss=0.0000 scale=2.0000 norm=1.1188\n",
      "== Quitting at iteration / GRAD 471\n",
      "[iter 0] loss=0.9279 val_loss=0.0000 scale=2.0000 norm=1.4474\n",
      "[iter 100] loss=-0.2807 val_loss=0.0000 scale=2.0000 norm=0.9594\n",
      "[iter 200] loss=-1.3408 val_loss=0.0000 scale=4.0000 norm=1.9872\n",
      "[iter 300] loss=-3.3406 val_loss=0.0000 scale=4.0000 norm=1.9997\n",
      "[iter 400] loss=-6.8606 val_loss=0.0000 scale=8.0000 norm=4.0000\n",
      "Exp 9\n",
      "Attempt Gibbs 1\n",
      "Iter Gibbs 0\n",
      "Iter Gibbs 100\n",
      "Iter Gibbs 200\n",
      "Iter Gibbs 300\n",
      "Iter Gibbs 400\n",
      "Iter Gibbs 500\n",
      "Iter Gibbs 600\n",
      "Iter Gibbs 700\n",
      "Iter Gibbs 800\n",
      "Iter Gibbs 900\n",
      "Iter Gibbs 1000\n",
      "Iter Gibbs 1100\n",
      "Iter Gibbs 1200\n",
      "Iter Gibbs 1300\n",
      "Iter Gibbs 1400\n",
      "Iter Gibbs 1500\n",
      "Iter Gibbs 1600\n",
      "Iter Gibbs 1700\n",
      "Iter Gibbs 1800\n",
      "Iter Gibbs 1900\n",
      "[iter 0] loss=0.9197 val_loss=0.0000 scale=2.0000 norm=1.4375\n",
      "[iter 100] loss=0.4107 val_loss=0.0000 scale=2.0000 norm=1.0544\n",
      "[iter 200] loss=0.3896 val_loss=0.0000 scale=2.0000 norm=1.1313\n",
      "[iter 300] loss=0.3891 val_loss=0.0000 scale=2.0000 norm=1.1490\n",
      "[iter 400] loss=0.3891 val_loss=0.0000 scale=2.0000 norm=1.1515\n",
      "== Quitting at iteration / GRAD 472\n",
      "[iter 0] loss=0.9197 val_loss=0.0000 scale=2.0000 norm=1.4375\n",
      "[iter 100] loss=-0.2900 val_loss=0.0000 scale=2.0000 norm=0.9604\n",
      "[iter 200] loss=-1.3602 val_loss=0.0000 scale=4.0000 norm=1.9875\n",
      "[iter 300] loss=-3.3600 val_loss=0.0000 scale=4.0000 norm=1.9997\n",
      "[iter 400] loss=-6.8000 val_loss=0.0000 scale=8.0000 norm=4.0000\n"
     ]
    }
   ],
   "source": [
    "N = 50\n",
    "opt = Contamination(n=N, lamda=0.0001)\n",
    "df = compute_empirical_coverage(opt, 10, test_set_size=0.2, nb=30)\n",
    "df.to_csv(\"acc_CON_ss50.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45f60e99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exp 0\n",
      "Attempt Gibbs 1\n",
      "Iter Gibbs 0\n",
      "Iter Gibbs 100\n",
      "Iter Gibbs 200\n",
      "Iter Gibbs 300\n",
      "Iter Gibbs 400\n",
      "Iter Gibbs 500\n",
      "Iter Gibbs 600\n",
      "Iter Gibbs 700\n",
      "Iter Gibbs 800\n",
      "Iter Gibbs 900\n",
      "Iter Gibbs 1000\n",
      "Iter Gibbs 1100\n",
      "Iter Gibbs 1200\n",
      "Iter Gibbs 1300\n",
      "Iter Gibbs 1400\n",
      "Iter Gibbs 1500\n",
      "Iter Gibbs 1600\n",
      "Iter Gibbs 1700\n",
      "Iter Gibbs 1800\n",
      "Iter Gibbs 1900\n",
      "[iter 0] loss=0.7873 val_loss=0.0000 scale=2.0000 norm=1.3413\n",
      "[iter 100] loss=0.2622 val_loss=0.0000 scale=2.0000 norm=0.9626\n",
      "[iter 200] loss=0.2301 val_loss=0.0000 scale=2.0000 norm=1.0339\n",
      "[iter 300] loss=0.2294 val_loss=0.0000 scale=2.0000 norm=1.0535\n",
      "[iter 400] loss=0.2293 val_loss=0.0000 scale=2.0000 norm=1.0564\n",
      "== Quitting at iteration / GRAD 481\n",
      "[iter 0] loss=0.7873 val_loss=0.0000 scale=1.0000 norm=0.6706\n",
      "[iter 100] loss=0.1947 val_loss=0.0000 scale=1.0000 norm=0.4568\n",
      "[iter 200] loss=-0.6467 val_loss=0.0000 scale=2.0000 norm=0.9006\n",
      "[iter 300] loss=-1.5797 val_loss=0.0000 scale=2.0000 norm=0.9295\n",
      "[iter 400] loss=-2.5123 val_loss=0.0000 scale=2.0000 norm=0.9317\n",
      "Exp 1\n",
      "Attempt Gibbs 1\n",
      "Iter Gibbs 0\n",
      "Iter Gibbs 100\n",
      "Iter Gibbs 200\n",
      "Iter Gibbs 300\n",
      "Iter Gibbs 400\n",
      "Iter Gibbs 500\n",
      "Iter Gibbs 600\n",
      "Iter Gibbs 700\n",
      "Iter Gibbs 800\n",
      "Iter Gibbs 900\n",
      "Iter Gibbs 1000\n",
      "Iter Gibbs 1100\n",
      "Iter Gibbs 1200\n",
      "Iter Gibbs 1300\n",
      "Iter Gibbs 1400\n",
      "Iter Gibbs 1500\n",
      "Iter Gibbs 1600\n",
      "Iter Gibbs 1700\n",
      "Iter Gibbs 1800\n",
      "Iter Gibbs 1900\n",
      "[iter 0] loss=0.8203 val_loss=0.0000 scale=2.0000 norm=1.3797\n",
      "[iter 100] loss=0.2785 val_loss=0.0000 scale=2.0000 norm=0.9953\n",
      "[iter 200] loss=0.2439 val_loss=0.0000 scale=2.0000 norm=1.0813\n",
      "[iter 300] loss=0.2430 val_loss=0.0000 scale=2.0000 norm=1.1044\n",
      "[iter 400] loss=0.2430 val_loss=0.0000 scale=2.0000 norm=1.1078\n",
      "== Quitting at iteration / GRAD 486\n",
      "[iter 0] loss=0.8203 val_loss=0.0000 scale=1.0000 norm=0.6899\n",
      "[iter 100] loss=0.1856 val_loss=0.0000 scale=2.0000 norm=0.9327\n",
      "[iter 200] loss=-0.6595 val_loss=0.0000 scale=2.0000 norm=0.9040\n",
      "[iter 300] loss=-1.5802 val_loss=0.0000 scale=2.0000 norm=0.9194\n",
      "[iter 400] loss=-2.5114 val_loss=0.0000 scale=2.0000 norm=0.9298\n",
      "Exp 2\n",
      "Attempt Gibbs 1\n",
      "Iter Gibbs 0\n",
      "Iter Gibbs 100\n",
      "Iter Gibbs 200\n",
      "Iter Gibbs 300\n",
      "Iter Gibbs 400\n",
      "Iter Gibbs 500\n",
      "Iter Gibbs 600\n",
      "Iter Gibbs 700\n",
      "Iter Gibbs 800\n",
      "Iter Gibbs 900\n",
      "Iter Gibbs 1000\n",
      "Iter Gibbs 1100\n",
      "Iter Gibbs 1200\n",
      "Iter Gibbs 1300\n",
      "Iter Gibbs 1400\n",
      "Iter Gibbs 1500\n",
      "Iter Gibbs 1600\n",
      "Iter Gibbs 1700\n",
      "Iter Gibbs 1800\n",
      "Iter Gibbs 1900\n",
      "[iter 0] loss=0.8275 val_loss=0.0000 scale=2.0000 norm=1.3955\n",
      "[iter 100] loss=0.2851 val_loss=0.0000 scale=2.0000 norm=1.0162\n",
      "[iter 200] loss=0.2519 val_loss=0.0000 scale=2.0000 norm=1.1029\n",
      "[iter 300] loss=0.2510 val_loss=0.0000 scale=2.0000 norm=1.1256\n",
      "[iter 400] loss=0.2510 val_loss=0.0000 scale=2.0000 norm=1.1289\n",
      "== Quitting at iteration / GRAD 484\n",
      "[iter 0] loss=0.8275 val_loss=0.0000 scale=1.0000 norm=0.6977\n",
      "[iter 100] loss=0.1996 val_loss=0.0000 scale=1.0000 norm=0.4591\n",
      "[iter 200] loss=-0.6539 val_loss=0.0000 scale=2.0000 norm=0.8944\n",
      "[iter 300] loss=-1.5634 val_loss=0.0000 scale=2.0000 norm=0.9084\n",
      "[iter 400] loss=-2.4931 val_loss=0.0000 scale=2.0000 norm=0.9288\n",
      "Exp 3\n",
      "Attempt Gibbs 1\n",
      "Iter Gibbs 0\n",
      "Iter Gibbs 100\n",
      "Iter Gibbs 200\n",
      "Iter Gibbs 300\n",
      "Iter Gibbs 400\n",
      "Iter Gibbs 500\n",
      "Iter Gibbs 600\n",
      "Iter Gibbs 700\n",
      "Iter Gibbs 800\n",
      "Iter Gibbs 900\n",
      "Iter Gibbs 1000\n",
      "Iter Gibbs 1100\n",
      "Iter Gibbs 1200\n",
      "Iter Gibbs 1300\n",
      "Iter Gibbs 1400\n",
      "Iter Gibbs 1500\n",
      "Iter Gibbs 1600\n",
      "Iter Gibbs 1700\n",
      "Iter Gibbs 1800\n",
      "Iter Gibbs 1900\n",
      "[iter 0] loss=0.7858 val_loss=0.0000 scale=2.0000 norm=1.3342\n",
      "[iter 100] loss=0.2198 val_loss=0.0000 scale=2.0000 norm=0.9683\n",
      "[iter 200] loss=0.1827 val_loss=0.0000 scale=2.0000 norm=1.0594\n",
      "[iter 300] loss=0.1816 val_loss=0.0000 scale=2.0000 norm=1.0848\n",
      "[iter 400] loss=0.1816 val_loss=0.0000 scale=2.0000 norm=1.0885\n",
      "== Quitting at iteration / GRAD 489\n",
      "[iter 0] loss=0.7858 val_loss=0.0000 scale=1.0000 norm=0.6671\n",
      "[iter 100] loss=0.1678 val_loss=0.0000 scale=2.0000 norm=0.8997\n",
      "[iter 200] loss=-0.7228 val_loss=0.0000 scale=2.0000 norm=0.8934\n",
      "[iter 300] loss=-1.6321 val_loss=0.0000 scale=2.0000 norm=0.9101\n",
      "[iter 400] loss=-2.5532 val_loss=0.0000 scale=2.0000 norm=0.9221\n",
      "Exp 4\n",
      "Attempt Gibbs 1\n",
      "Iter Gibbs 0\n",
      "Iter Gibbs 100\n",
      "Iter Gibbs 200\n",
      "Iter Gibbs 300\n",
      "Iter Gibbs 400\n",
      "Iter Gibbs 500\n",
      "Iter Gibbs 600\n",
      "Iter Gibbs 700\n",
      "Iter Gibbs 800\n",
      "Iter Gibbs 900\n",
      "Iter Gibbs 1000\n",
      "Iter Gibbs 1100\n",
      "Iter Gibbs 1200\n",
      "Iter Gibbs 1300\n",
      "Iter Gibbs 1400\n",
      "Iter Gibbs 1500\n",
      "Iter Gibbs 1600\n",
      "Iter Gibbs 1700\n",
      "Iter Gibbs 1800\n",
      "Iter Gibbs 1900\n",
      "[iter 0] loss=0.8247 val_loss=0.0000 scale=2.0000 norm=1.3747\n",
      "[iter 100] loss=0.2258 val_loss=0.0000 scale=2.0000 norm=0.9672\n",
      "[iter 200] loss=0.1842 val_loss=0.0000 scale=2.0000 norm=1.0600\n",
      "[iter 300] loss=0.1830 val_loss=0.0000 scale=2.0000 norm=1.0870\n",
      "[iter 400] loss=0.1830 val_loss=0.0000 scale=2.0000 norm=1.0911\n",
      "== Quitting at iteration / GRAD 494\n",
      "[iter 0] loss=0.8247 val_loss=0.0000 scale=1.0000 norm=0.6874\n",
      "[iter 100] loss=0.2220 val_loss=0.0000 scale=1.0000 norm=0.4622\n",
      "[iter 200] loss=-0.6478 val_loss=0.0000 scale=2.0000 norm=0.9053\n",
      "[iter 300] loss=-1.5846 val_loss=0.0000 scale=2.0000 norm=0.9340\n",
      "[iter 400] loss=-2.5258 val_loss=0.0000 scale=2.0000 norm=0.9392\n",
      "Exp 5\n",
      "Attempt Gibbs 1\n",
      "Iter Gibbs 0\n",
      "Iter Gibbs 100\n",
      "Iter Gibbs 200\n",
      "Iter Gibbs 300\n",
      "Iter Gibbs 400\n",
      "Iter Gibbs 500\n",
      "Iter Gibbs 600\n",
      "Iter Gibbs 700\n",
      "Iter Gibbs 800\n",
      "Iter Gibbs 900\n",
      "Iter Gibbs 1000\n",
      "Iter Gibbs 1100\n",
      "Iter Gibbs 1200\n",
      "Iter Gibbs 1300\n",
      "Iter Gibbs 1400\n",
      "Iter Gibbs 1500\n",
      "Iter Gibbs 1600\n",
      "Iter Gibbs 1700\n",
      "Iter Gibbs 1800\n",
      "Iter Gibbs 1900\n",
      "[iter 0] loss=0.7943 val_loss=0.0000 scale=2.0000 norm=1.3705\n",
      "[iter 100] loss=0.1741 val_loss=0.0000 scale=2.0000 norm=0.9792\n",
      "[iter 200] loss=0.1261 val_loss=0.0000 scale=2.0000 norm=1.0881\n",
      "[iter 300] loss=0.1246 val_loss=0.0000 scale=2.0000 norm=1.1195\n",
      "[iter 400] loss=0.1245 val_loss=0.0000 scale=2.0000 norm=1.1242\n",
      "== Quitting at iteration / GRAD 498\n",
      "[iter 0] loss=0.7943 val_loss=0.0000 scale=1.0000 norm=0.6852\n",
      "[iter 100] loss=0.1545 val_loss=0.0000 scale=1.0000 norm=0.4584\n",
      "[iter 200] loss=-0.7464 val_loss=0.0000 scale=2.0000 norm=0.9017\n",
      "[iter 300] loss=-1.6715 val_loss=0.0000 scale=2.0000 norm=0.9229\n",
      "[iter 400] loss=-2.6010 val_loss=0.0000 scale=2.0000 norm=0.9282\n",
      "Exp 6\n",
      "Attempt Gibbs 1\n",
      "Iter Gibbs 0\n",
      "Iter Gibbs 100\n",
      "Iter Gibbs 200\n",
      "Iter Gibbs 300\n",
      "Iter Gibbs 400\n",
      "Iter Gibbs 500\n",
      "Iter Gibbs 600\n",
      "Iter Gibbs 700\n",
      "Iter Gibbs 800\n",
      "Iter Gibbs 900\n",
      "Iter Gibbs 1000\n",
      "Iter Gibbs 1100\n",
      "Iter Gibbs 1200\n",
      "Iter Gibbs 1300\n",
      "Iter Gibbs 1400\n",
      "Iter Gibbs 1500\n",
      "Iter Gibbs 1600\n",
      "Iter Gibbs 1700\n",
      "Iter Gibbs 1800\n",
      "Iter Gibbs 1900\n",
      "[iter 0] loss=0.7890 val_loss=0.0000 scale=2.0000 norm=1.3677\n",
      "[iter 100] loss=0.2437 val_loss=0.0000 scale=2.0000 norm=1.0001\n",
      "[iter 200] loss=0.2117 val_loss=0.0000 scale=2.0000 norm=1.0947\n",
      "[iter 300] loss=0.2108 val_loss=0.0000 scale=2.0000 norm=1.1186\n",
      "[iter 400] loss=0.2108 val_loss=0.0000 scale=2.0000 norm=1.1221\n",
      "== Quitting at iteration / GRAD 485\n",
      "[iter 0] loss=0.7890 val_loss=0.0000 scale=1.0000 norm=0.6839\n",
      "[iter 100] loss=0.1903 val_loss=0.0000 scale=1.0000 norm=0.4613\n",
      "[iter 200] loss=-0.5763 val_loss=0.0000 scale=2.0000 norm=0.9014\n",
      "[iter 300] loss=-1.5043 val_loss=0.0000 scale=2.0000 norm=0.9258\n",
      "[iter 400] loss=-2.4388 val_loss=0.0000 scale=2.0000 norm=0.9323\n",
      "Exp 7\n",
      "Attempt Gibbs 1\n",
      "Iter Gibbs 0\n",
      "Iter Gibbs 100\n",
      "Iter Gibbs 200\n",
      "Iter Gibbs 300\n",
      "Iter Gibbs 400\n",
      "Iter Gibbs 500\n",
      "Iter Gibbs 600\n",
      "Iter Gibbs 700\n",
      "Iter Gibbs 800\n",
      "Iter Gibbs 900\n",
      "Iter Gibbs 1000\n",
      "Iter Gibbs 1100\n",
      "Iter Gibbs 1200\n",
      "Iter Gibbs 1300\n",
      "Iter Gibbs 1400\n",
      "Iter Gibbs 1500\n",
      "Iter Gibbs 1600\n",
      "Iter Gibbs 1700\n",
      "Iter Gibbs 1800\n",
      "Iter Gibbs 1900\n",
      "[iter 0] loss=0.7738 val_loss=0.0000 scale=2.0000 norm=1.3573\n",
      "[iter 100] loss=0.2283 val_loss=0.0000 scale=2.0000 norm=0.9637\n",
      "[iter 200] loss=0.1982 val_loss=0.0000 scale=2.0000 norm=1.0440\n",
      "[iter 300] loss=0.1974 val_loss=0.0000 scale=2.0000 norm=1.0650\n",
      "[iter 400] loss=0.1974 val_loss=0.0000 scale=2.0000 norm=1.0681\n",
      "== Quitting at iteration / GRAD 483\n",
      "[iter 0] loss=0.7738 val_loss=0.0000 scale=1.0000 norm=0.6786\n",
      "[iter 100] loss=0.1821 val_loss=0.0000 scale=1.0000 norm=0.4571\n",
      "[iter 200] loss=-0.6095 val_loss=0.0000 scale=2.0000 norm=0.8903\n",
      "[iter 300] loss=-1.5206 val_loss=0.0000 scale=2.0000 norm=0.9127\n",
      "[iter 400] loss=-2.4422 val_loss=0.0000 scale=2.0000 norm=0.9203\n",
      "Exp 8\n",
      "Attempt Gibbs 1\n",
      "Iter Gibbs 0\n",
      "Iter Gibbs 100\n",
      "Iter Gibbs 200\n",
      "Iter Gibbs 300\n",
      "Iter Gibbs 400\n",
      "Iter Gibbs 500\n",
      "Iter Gibbs 600\n",
      "Iter Gibbs 700\n",
      "Iter Gibbs 800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter Gibbs 900\n",
      "Iter Gibbs 1000\n",
      "Iter Gibbs 1100\n",
      "Iter Gibbs 1200\n",
      "Iter Gibbs 1300\n",
      "Iter Gibbs 1400\n",
      "Iter Gibbs 1500\n",
      "Iter Gibbs 1600\n",
      "Iter Gibbs 1700\n",
      "Iter Gibbs 1800\n",
      "Iter Gibbs 1900\n",
      "[iter 0] loss=0.8313 val_loss=0.0000 scale=2.0000 norm=1.3814\n",
      "[iter 100] loss=0.2422 val_loss=0.0000 scale=2.0000 norm=0.9844\n",
      "[iter 200] loss=0.1989 val_loss=0.0000 scale=2.0000 norm=1.0795\n",
      "[iter 300] loss=0.1976 val_loss=0.0000 scale=2.0000 norm=1.1074\n",
      "[iter 400] loss=0.1976 val_loss=0.0000 scale=2.0000 norm=1.1115\n",
      "== Quitting at iteration / GRAD 494\n",
      "[iter 0] loss=0.8313 val_loss=0.0000 scale=1.0000 norm=0.6907\n",
      "[iter 100] loss=0.2385 val_loss=0.0000 scale=1.0000 norm=0.4662\n",
      "[iter 200] loss=-0.5177 val_loss=0.0000 scale=2.0000 norm=0.8993\n",
      "[iter 300] loss=-1.4378 val_loss=0.0000 scale=2.0000 norm=0.9193\n",
      "[iter 400] loss=-2.3721 val_loss=0.0000 scale=2.0000 norm=0.9330\n",
      "Exp 9\n",
      "Attempt Gibbs 1\n",
      "Iter Gibbs 0\n",
      "Iter Gibbs 100\n",
      "Iter Gibbs 200\n",
      "Iter Gibbs 300\n",
      "Iter Gibbs 400\n",
      "Iter Gibbs 500\n",
      "Iter Gibbs 600\n",
      "Iter Gibbs 700\n",
      "Iter Gibbs 800\n",
      "Iter Gibbs 900\n",
      "Iter Gibbs 1000\n",
      "Iter Gibbs 1100\n",
      "Iter Gibbs 1200\n",
      "Iter Gibbs 1300\n",
      "Iter Gibbs 1400\n",
      "Iter Gibbs 1500\n",
      "Iter Gibbs 1600\n",
      "Iter Gibbs 1700\n",
      "Iter Gibbs 1800\n",
      "Iter Gibbs 1900\n",
      "[iter 0] loss=0.7986 val_loss=0.0000 scale=2.0000 norm=1.3834\n",
      "[iter 100] loss=0.2552 val_loss=0.0000 scale=2.0000 norm=0.9916\n",
      "[iter 200] loss=0.2255 val_loss=0.0000 scale=2.0000 norm=1.0754\n",
      "[iter 300] loss=0.2247 val_loss=0.0000 scale=2.0000 norm=1.0968\n",
      "[iter 400] loss=0.2247 val_loss=0.0000 scale=2.0000 norm=1.0999\n",
      "== Quitting at iteration / GRAD 483\n",
      "[iter 0] loss=0.7986 val_loss=0.0000 scale=1.0000 norm=0.6917\n",
      "[iter 100] loss=0.1899 val_loss=0.0000 scale=1.0000 norm=0.4627\n",
      "[iter 200] loss=-0.6302 val_loss=0.0000 scale=2.0000 norm=0.8996\n",
      "[iter 300] loss=-1.5464 val_loss=0.0000 scale=2.0000 norm=0.9146\n",
      "[iter 400] loss=-2.4692 val_loss=0.0000 scale=2.0000 norm=0.9245\n"
     ]
    }
   ],
   "source": [
    "N = 200\n",
    "opt = Contamination(n=N, lamda=0.0001)\n",
    "\n",
    "df = compute_empirical_coverage(opt, 10, test_set_size=0.2, nb=30)\n",
    "\n",
    "df.to_csv(\"acc_CON_ss200.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e422935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exp 0\n",
      "Attempt Gibbs 1\n",
      "Iter Gibbs 0\n",
      "Iter Gibbs 100\n",
      "Iter Gibbs 200\n",
      "Iter Gibbs 300\n",
      "Iter Gibbs 400\n",
      "Iter Gibbs 500\n",
      "Iter Gibbs 600\n",
      "Iter Gibbs 700\n",
      "Iter Gibbs 800\n",
      "Iter Gibbs 900\n",
      "Iter Gibbs 1000\n",
      "Iter Gibbs 1100\n",
      "Iter Gibbs 1200\n",
      "Iter Gibbs 1300\n",
      "Iter Gibbs 1400\n",
      "Iter Gibbs 1500\n",
      "Iter Gibbs 1600\n",
      "Iter Gibbs 1700\n",
      "Iter Gibbs 1800\n",
      "Iter Gibbs 1900\n",
      "[iter 0] loss=0.7643 val_loss=0.0000 scale=2.0000 norm=1.3545\n",
      "[iter 100] loss=0.1567 val_loss=0.0000 scale=2.0000 norm=0.9565\n",
      "[iter 200] loss=0.1089 val_loss=0.0000 scale=2.0000 norm=1.0630\n",
      "[iter 300] loss=0.1074 val_loss=0.0000 scale=2.0000 norm=1.0944\n",
      "[iter 400] loss=0.1074 val_loss=0.0000 scale=2.0000 norm=1.0991\n",
      "== Quitting at iteration / GRAD 498\n",
      "[iter 0] loss=0.7643 val_loss=0.0000 scale=1.0000 norm=0.6773\n",
      "[iter 100] loss=0.2677 val_loss=0.0000 scale=1.0000 norm=0.4630\n",
      "[iter 200] loss=-0.1141 val_loss=0.0000 scale=1.0000 norm=0.4220\n",
      "[iter 300] loss=-0.4878 val_loss=0.0000 scale=1.0000 norm=0.4051\n",
      "[iter 400] loss=-0.8517 val_loss=0.0000 scale=1.0000 norm=0.3891\n",
      "Exp 1\n",
      "Attempt Gibbs 1\n",
      "Iter Gibbs 0\n",
      "Iter Gibbs 100\n",
      "Iter Gibbs 200\n",
      "Iter Gibbs 300\n",
      "Iter Gibbs 400\n",
      "Iter Gibbs 500\n",
      "Iter Gibbs 600\n",
      "Iter Gibbs 700\n",
      "Iter Gibbs 800\n",
      "Iter Gibbs 900\n",
      "Iter Gibbs 1000\n",
      "Iter Gibbs 1100\n",
      "Iter Gibbs 1200\n",
      "Iter Gibbs 1300\n",
      "Iter Gibbs 1400\n",
      "Iter Gibbs 1500\n",
      "Iter Gibbs 1600\n",
      "Iter Gibbs 1700\n",
      "Iter Gibbs 1800\n",
      "Iter Gibbs 1900\n",
      "[iter 0] loss=0.7705 val_loss=0.0000 scale=2.0000 norm=1.3521\n",
      "[iter 100] loss=0.1838 val_loss=0.0000 scale=2.0000 norm=0.9744\n",
      "[iter 200] loss=0.1408 val_loss=0.0000 scale=2.0000 norm=1.0754\n",
      "[iter 300] loss=0.1395 val_loss=0.0000 scale=2.0000 norm=1.1044\n",
      "[iter 400] loss=0.1395 val_loss=0.0000 scale=2.0000 norm=1.1087\n",
      "== Quitting at iteration / GRAD 494\n",
      "[iter 0] loss=0.7705 val_loss=0.0000 scale=1.0000 norm=0.6761\n",
      "[iter 100] loss=0.2771 val_loss=0.0000 scale=1.0000 norm=0.4663\n",
      "[iter 200] loss=-0.1039 val_loss=0.0000 scale=1.0000 norm=0.4231\n",
      "[iter 300] loss=-0.4874 val_loss=0.0000 scale=1.0000 norm=0.4018\n",
      "[iter 400] loss=-0.8742 val_loss=0.0000 scale=1.0000 norm=0.3890\n",
      "Exp 2\n",
      "Attempt Gibbs 1\n",
      "Iter Gibbs 0\n",
      "Iter Gibbs 100\n",
      "Iter Gibbs 200\n",
      "Iter Gibbs 300\n",
      "Iter Gibbs 400\n",
      "Iter Gibbs 500\n",
      "Iter Gibbs 600\n",
      "Iter Gibbs 700\n",
      "Iter Gibbs 800\n",
      "Iter Gibbs 900\n",
      "Iter Gibbs 1000\n",
      "Iter Gibbs 1100\n",
      "Iter Gibbs 1200\n",
      "Iter Gibbs 1300\n",
      "Iter Gibbs 1400\n",
      "Iter Gibbs 1500\n",
      "Iter Gibbs 1600\n",
      "Iter Gibbs 1700\n",
      "Iter Gibbs 1800\n",
      "Iter Gibbs 1900\n",
      "[iter 0] loss=0.7867 val_loss=0.0000 scale=2.0000 norm=1.3713\n",
      "[iter 100] loss=0.1636 val_loss=0.0000 scale=2.0000 norm=0.9725\n",
      "[iter 200] loss=0.1123 val_loss=0.0000 scale=2.0000 norm=1.0898\n",
      "[iter 300] loss=0.1107 val_loss=0.0000 scale=2.0000 norm=1.1243\n",
      "[iter 400] loss=0.1107 val_loss=0.0000 scale=2.0000 norm=1.1295\n",
      "[iter 0] loss=0.7867 val_loss=0.0000 scale=1.0000 norm=0.6857\n",
      "[iter 100] loss=0.2811 val_loss=0.0000 scale=1.0000 norm=0.4711\n",
      "[iter 200] loss=-0.0940 val_loss=0.0000 scale=1.0000 norm=0.4278\n",
      "[iter 300] loss=-0.4724 val_loss=0.0000 scale=1.0000 norm=0.4085\n",
      "[iter 400] loss=-0.8382 val_loss=0.0000 scale=1.0000 norm=0.3939\n",
      "Exp 3\n",
      "Attempt Gibbs 1\n",
      "Iter Gibbs 0\n",
      "Iter Gibbs 100\n",
      "Iter Gibbs 200\n",
      "Iter Gibbs 300\n",
      "Iter Gibbs 400\n",
      "Iter Gibbs 500\n",
      "Iter Gibbs 600\n",
      "Iter Gibbs 700\n",
      "Iter Gibbs 800\n",
      "Iter Gibbs 900\n",
      "Iter Gibbs 1000\n",
      "Iter Gibbs 1100\n",
      "Iter Gibbs 1200\n",
      "Iter Gibbs 1300\n",
      "Iter Gibbs 1400\n",
      "Iter Gibbs 1500\n",
      "Iter Gibbs 1600\n",
      "Iter Gibbs 1700\n",
      "Iter Gibbs 1800\n",
      "Iter Gibbs 1900\n",
      "[iter 0] loss=0.7486 val_loss=0.0000 scale=2.0000 norm=1.3425\n",
      "[iter 100] loss=0.1876 val_loss=0.0000 scale=2.0000 norm=0.9764\n",
      "[iter 200] loss=0.1526 val_loss=0.0000 scale=2.0000 norm=1.0721\n",
      "[iter 300] loss=0.1516 val_loss=0.0000 scale=2.0000 norm=1.0973\n",
      "[iter 400] loss=0.1516 val_loss=0.0000 scale=2.0000 norm=1.1010\n",
      "== Quitting at iteration / GRAD 488\n",
      "[iter 0] loss=0.7486 val_loss=0.0000 scale=1.0000 norm=0.6713\n",
      "[iter 100] loss=0.2658 val_loss=0.0000 scale=1.0000 norm=0.4671\n",
      "[iter 200] loss=-0.1123 val_loss=0.0000 scale=1.0000 norm=0.4216\n",
      "[iter 300] loss=-0.4783 val_loss=0.0000 scale=1.0000 norm=0.3983\n",
      "[iter 400] loss=-0.8345 val_loss=0.0000 scale=1.0000 norm=0.3821\n",
      "Exp 4\n",
      "Attempt Gibbs 1\n",
      "Iter Gibbs 0\n",
      "Iter Gibbs 100\n",
      "Iter Gibbs 200\n",
      "Iter Gibbs 300\n",
      "Iter Gibbs 400\n",
      "Iter Gibbs 500\n",
      "Iter Gibbs 600\n",
      "Iter Gibbs 700\n",
      "Iter Gibbs 800\n",
      "Iter Gibbs 900\n",
      "Iter Gibbs 1000\n",
      "Iter Gibbs 1100\n",
      "Iter Gibbs 1200\n",
      "Iter Gibbs 1300\n",
      "Iter Gibbs 1400\n",
      "Iter Gibbs 1500\n",
      "Iter Gibbs 1600\n",
      "Iter Gibbs 1700\n",
      "Iter Gibbs 1800\n",
      "Iter Gibbs 1900\n",
      "[iter 0] loss=0.7828 val_loss=0.0000 scale=2.0000 norm=1.3557\n",
      "[iter 100] loss=0.1630 val_loss=0.0000 scale=2.0000 norm=0.9462\n",
      "[iter 200] loss=0.1054 val_loss=0.0000 scale=2.0000 norm=1.0466\n",
      "[iter 300] loss=0.1036 val_loss=0.0000 scale=2.0000 norm=1.0807\n",
      "[iter 400] loss=0.1035 val_loss=0.0000 scale=2.0000 norm=1.0858\n",
      "[iter 0] loss=0.7828 val_loss=0.0000 scale=1.0000 norm=0.6779\n",
      "[iter 100] loss=0.2903 val_loss=0.0000 scale=1.0000 norm=0.4659\n",
      "[iter 200] loss=-0.1093 val_loss=0.0000 scale=1.0000 norm=0.4207\n",
      "[iter 300] loss=-0.5449 val_loss=0.0000 scale=1.0000 norm=0.4001\n",
      "[iter 400] loss=-0.9710 val_loss=0.0000 scale=1.0000 norm=0.3848\n",
      "Exp 5\n",
      "Attempt Gibbs 1\n",
      "Iter Gibbs 0\n",
      "Iter Gibbs 100\n",
      "Iter Gibbs 200\n",
      "Iter Gibbs 300\n",
      "Iter Gibbs 400\n",
      "Iter Gibbs 500\n",
      "Iter Gibbs 600\n",
      "Iter Gibbs 700\n",
      "Iter Gibbs 800\n",
      "Iter Gibbs 900\n",
      "Iter Gibbs 1000\n",
      "Iter Gibbs 1100\n",
      "Iter Gibbs 1200\n",
      "Iter Gibbs 1300\n",
      "Iter Gibbs 1400\n",
      "Iter Gibbs 1500\n",
      "Iter Gibbs 1600\n",
      "Iter Gibbs 1700\n",
      "Iter Gibbs 1800\n",
      "Iter Gibbs 1900\n",
      "[iter 0] loss=0.7157 val_loss=0.0000 scale=2.0000 norm=1.3308\n",
      "[iter 100] loss=0.1659 val_loss=0.0000 scale=2.0000 norm=0.9638\n",
      "[iter 200] loss=0.1317 val_loss=0.0000 scale=2.0000 norm=1.0607\n",
      "[iter 300] loss=0.1307 val_loss=0.0000 scale=2.0000 norm=1.0862\n",
      "[iter 400] loss=0.1307 val_loss=0.0000 scale=2.0000 norm=1.0899\n",
      "== Quitting at iteration / GRAD 487\n",
      "[iter 0] loss=0.7157 val_loss=0.0000 scale=1.0000 norm=0.6654\n",
      "[iter 100] loss=0.2363 val_loss=0.0000 scale=1.0000 norm=0.4620\n",
      "[iter 200] loss=-0.1394 val_loss=0.0000 scale=1.0000 norm=0.4171\n",
      "[iter 300] loss=-0.4984 val_loss=0.0000 scale=1.0000 norm=0.3922\n",
      "[iter 400] loss=-0.8433 val_loss=0.0000 scale=1.0000 norm=0.3723\n",
      "Exp 6\n",
      "Attempt Gibbs 1\n",
      "Iter Gibbs 0\n",
      "Iter Gibbs 100\n",
      "Iter Gibbs 200\n",
      "Iter Gibbs 300\n",
      "Iter Gibbs 400\n",
      "Iter Gibbs 500\n",
      "Iter Gibbs 600\n",
      "Iter Gibbs 700\n",
      "Iter Gibbs 800\n",
      "Iter Gibbs 900\n",
      "Iter Gibbs 1000\n",
      "Iter Gibbs 1100\n",
      "Iter Gibbs 1200\n",
      "Iter Gibbs 1300\n",
      "Iter Gibbs 1400\n",
      "Iter Gibbs 1500\n",
      "Iter Gibbs 1600\n",
      "Iter Gibbs 1700\n",
      "Iter Gibbs 1800\n",
      "Iter Gibbs 1900\n",
      "[iter 0] loss=0.7381 val_loss=0.0000 scale=2.0000 norm=1.3433\n",
      "[iter 100] loss=0.1698 val_loss=0.0000 scale=2.0000 norm=0.9662\n",
      "[iter 200] loss=0.1317 val_loss=0.0000 scale=2.0000 norm=1.0625\n",
      "[iter 300] loss=0.1306 val_loss=0.0000 scale=2.0000 norm=1.0888\n",
      "[iter 400] loss=0.1306 val_loss=0.0000 scale=2.0000 norm=1.0927\n",
      "== Quitting at iteration / GRAD 490\n",
      "[iter 0] loss=0.7381 val_loss=0.0000 scale=1.0000 norm=0.6716\n",
      "[iter 100] loss=0.2557 val_loss=0.0000 scale=1.0000 norm=0.4602\n",
      "[iter 200] loss=-0.1142 val_loss=0.0000 scale=1.0000 norm=0.4183\n",
      "[iter 300] loss=-0.4878 val_loss=0.0000 scale=1.0000 norm=0.3994\n",
      "[iter 400] loss=-0.8549 val_loss=0.0000 scale=1.0000 norm=0.3804\n",
      "Exp 7\n",
      "Attempt Gibbs 1\n",
      "Iter Gibbs 0\n",
      "Iter Gibbs 100\n",
      "Iter Gibbs 200\n",
      "Iter Gibbs 300\n",
      "Iter Gibbs 400\n",
      "Iter Gibbs 500\n",
      "Iter Gibbs 600\n",
      "Iter Gibbs 700\n",
      "Iter Gibbs 800\n",
      "Iter Gibbs 900\n",
      "Iter Gibbs 1000\n",
      "Iter Gibbs 1100\n",
      "Iter Gibbs 1200\n",
      "Iter Gibbs 1300\n",
      "Iter Gibbs 1400\n",
      "Iter Gibbs 1500\n",
      "Iter Gibbs 1600\n",
      "Iter Gibbs 1700\n",
      "Iter Gibbs 1800\n",
      "Iter Gibbs 1900\n",
      "[iter 0] loss=0.7747 val_loss=0.0000 scale=2.0000 norm=1.3536\n",
      "[iter 100] loss=0.1713 val_loss=0.0000 scale=2.0000 norm=0.9508\n",
      "[iter 200] loss=0.1181 val_loss=0.0000 scale=2.0000 norm=1.0516\n",
      "[iter 300] loss=0.1165 val_loss=0.0000 scale=2.0000 norm=1.0842\n",
      "[iter 400] loss=0.1165 val_loss=0.0000 scale=2.0000 norm=1.0891\n",
      "[iter 0] loss=0.7747 val_loss=0.0000 scale=1.0000 norm=0.6768\n",
      "[iter 100] loss=0.2883 val_loss=0.0000 scale=1.0000 norm=0.4684\n",
      "[iter 200] loss=-0.0781 val_loss=0.0000 scale=1.0000 norm=0.4207\n",
      "[iter 300] loss=-0.4393 val_loss=0.0000 scale=1.0000 norm=0.3971\n",
      "[iter 400] loss=-0.7975 val_loss=0.0000 scale=1.0000 norm=0.3823\n",
      "Exp 8\n",
      "Attempt Gibbs 1\n",
      "Iter Gibbs 0\n",
      "Iter Gibbs 100\n",
      "Iter Gibbs 200\n",
      "Iter Gibbs 300\n",
      "Iter Gibbs 400\n",
      "Iter Gibbs 500\n",
      "Iter Gibbs 600\n",
      "Iter Gibbs 700\n",
      "Iter Gibbs 800\n",
      "Iter Gibbs 900\n",
      "Iter Gibbs 1000\n",
      "Iter Gibbs 1100\n",
      "Iter Gibbs 1200\n",
      "Iter Gibbs 1300\n",
      "Iter Gibbs 1400\n",
      "Iter Gibbs 1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter Gibbs 1600\n",
      "Iter Gibbs 1700\n",
      "Iter Gibbs 1800\n",
      "Iter Gibbs 1900\n",
      "[iter 0] loss=0.7473 val_loss=0.0000 scale=2.0000 norm=1.3433\n",
      "[iter 100] loss=0.1782 val_loss=0.0000 scale=2.0000 norm=0.9590\n",
      "[iter 200] loss=0.1368 val_loss=0.0000 scale=2.0000 norm=1.0511\n",
      "[iter 300] loss=0.1356 val_loss=0.0000 scale=2.0000 norm=1.0782\n",
      "[iter 400] loss=0.1356 val_loss=0.0000 scale=2.0000 norm=1.0822\n",
      "== Quitting at iteration / GRAD 492\n",
      "[iter 0] loss=0.7473 val_loss=0.0000 scale=1.0000 norm=0.6716\n",
      "[iter 100] loss=0.2464 val_loss=0.0000 scale=1.0000 norm=0.4586\n",
      "[iter 200] loss=-0.1388 val_loss=0.0000 scale=1.0000 norm=0.4139\n",
      "[iter 300] loss=-0.5902 val_loss=0.0000 scale=1.0000 norm=0.3924\n",
      "[iter 400] loss=-0.9540 val_loss=0.0000 scale=1.0000 norm=0.3776\n",
      "Exp 9\n",
      "Attempt Gibbs 1\n",
      "Iter Gibbs 0\n",
      "Iter Gibbs 100\n",
      "Iter Gibbs 200\n",
      "Iter Gibbs 300\n",
      "Iter Gibbs 400\n",
      "Iter Gibbs 500\n",
      "Iter Gibbs 600\n",
      "Iter Gibbs 700\n",
      "Iter Gibbs 800\n",
      "Iter Gibbs 900\n",
      "Iter Gibbs 1000\n",
      "Iter Gibbs 1100\n",
      "Iter Gibbs 1200\n",
      "Iter Gibbs 1300\n",
      "Iter Gibbs 1400\n",
      "Iter Gibbs 1500\n",
      "Iter Gibbs 1600\n",
      "Iter Gibbs 1700\n",
      "Iter Gibbs 1800\n",
      "Iter Gibbs 1900\n",
      "[iter 0] loss=0.7665 val_loss=0.0000 scale=2.0000 norm=1.3653\n",
      "[iter 100] loss=0.1764 val_loss=0.0000 scale=2.0000 norm=0.9680\n",
      "[iter 200] loss=0.1310 val_loss=0.0000 scale=2.0000 norm=1.0759\n",
      "[iter 300] loss=0.1297 val_loss=0.0000 scale=2.0000 norm=1.1066\n",
      "[iter 400] loss=0.1296 val_loss=0.0000 scale=2.0000 norm=1.1112\n",
      "== Quitting at iteration / GRAD 496\n",
      "[iter 0] loss=0.7665 val_loss=0.0000 scale=1.0000 norm=0.6826\n",
      "[iter 100] loss=0.2552 val_loss=0.0000 scale=1.0000 norm=0.4662\n",
      "[iter 200] loss=-0.1207 val_loss=0.0000 scale=1.0000 norm=0.4243\n",
      "[iter 300] loss=-0.4796 val_loss=0.0000 scale=1.0000 norm=0.3997\n",
      "[iter 400] loss=-0.8346 val_loss=0.0000 scale=1.0000 norm=0.3837\n"
     ]
    }
   ],
   "source": [
    "N = 400\n",
    "opt = Contamination(n=N, lamda=0.0001)\n",
    "\n",
    "df = compute_empirical_coverage(opt, 10, test_set_size=0.2, nb=30)\n",
    "\n",
    "df.to_csv(\"acc_CON_ss400.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66839fea",
   "metadata": {},
   "source": [
    "# TRASH - Accuracy More"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f45612ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 500\n",
    "opt = Contamination(n=N, lamda=0.0001)\n",
    "X = opt.X\n",
    "y = opt.y\n",
    "test_set_size = 0.2\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_set_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be302e36",
   "metadata": {},
   "source": [
    "## GP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "881a694f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test R^2: 0.754\n",
      "Test RMSE: 0.227 units\n",
      "Test MAE: 0.181 units\n"
     ]
    }
   ],
   "source": [
    "m1 = GPr()\n",
    "m1.fit(X_train, y_train)\n",
    "y_pred = m1.predict(X_test)\n",
    "y_pred_d = m1.pred_dist(X_test)\n",
    "\n",
    "# Output R^2, RMSE and MAE on the test set\n",
    "score = r2_score(y_test, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "print(\"\\nTest R^2: {:.3f}\".format(score))\n",
    "print(\"Test RMSE: {:.3f} units\".format(rmse))\n",
    "print(\"Test MAE: {:.3f} units\".format(mae))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "9ba0ee60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.         0.05263158 0.10526316 0.15789474 0.21052632 0.26315789\n",
      " 0.31578947 0.36842105 0.42105263 0.47368421 0.52631579 0.57894737\n",
      " 0.63157895 0.68421053 0.73684211 0.78947368 0.84210526 0.89473684\n",
      " 0.94736842 1.        ]\n",
      "[0.   0.08 0.16 0.26 0.35 0.45 0.51 0.59 0.68 0.76 0.81 0.84 0.85 0.87\n",
      " 0.94 0.96 0.96 0.98 1.   1.  ]\n"
     ]
    }
   ],
   "source": [
    "scorer = CVPPDiagram()\n",
    "qs, Cqs = scorer.compute(y_test, y_pred_d.mean(), y_pred_d.std(), num_bins=20)\n",
    "print(qs)\n",
    "print(Cqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d5232e04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11412973222530007"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scorer = AbsoluteMiscalibrationArea()\n",
    "scorer.compute(y_test, y_pred_d.mean(), y_pred_d.var, num_bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "46387466",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.        , 0.05263158, 0.10526316, 0.15789474, 0.21052632,\n",
       "        0.26315789, 0.31578947, 0.36842105, 0.42105263, 0.47368421,\n",
       "        0.52631579, 0.57894737, 0.63157895, 0.68421053, 0.73684211,\n",
       "        0.78947368, 0.84210526, 0.89473684, 0.94736842, 1.        ]),\n",
       " array([0.03, 0.09, 0.15, 0.23, 0.4 , 0.45, 0.51, 0.61, 0.69, 0.74, 0.81,\n",
       "        0.82, 0.85, 0.91, 0.93, 0.96, 0.96, 0.99, 1.  , 1.  ]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scorer = CoveragePlot()\n",
    "scorer.compute(y_test, y_pred_d, num_bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "10fd3abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_intervals(predictions, q):\n",
    "\n",
    "    low = np.zeros(len(predictions))\n",
    "    up = np.zeros(len(predictions))\n",
    "\n",
    "    for i, pred in enumerate(predictions):\n",
    "        sample = pred.sample(10**3)\n",
    "        low[i] = np.quantile(sample, (1.0-q)/2.0)\n",
    "        up[i]  = np.quantile(sample, 1.0 - (1.0-q)/2.0)\n",
    "\n",
    "    return low, up\n",
    "\n",
    "def empirical_coverage(y_true, y_pred_d, q):\n",
    "\n",
    "    low, up = compute_intervals(y_pred_d, q)\n",
    "    comp = np.logical_and((y_true < up) , (y_true > low))\n",
    "\n",
    "    return np.mean(comp)\n",
    "\n",
    "def compute(y_true, y_pred_d, num_bins=10):\n",
    "    qs = np.linspace(0, 1, num_bins)\n",
    "    Cqs = np.empty(qs.shape)\n",
    "    for ix, q in enumerate(qs):\n",
    "        Cqs[ix] = empirical_coverage(y_true, y_pred_d, q)\n",
    "\n",
    "    return qs, Cqs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "16182df2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[89], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred_d\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_bins\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[88], line 24\u001b[0m, in \u001b[0;36mcompute\u001b[0;34m(y_true, y_pred_d, num_bins)\u001b[0m\n\u001b[1;32m     22\u001b[0m Cqs \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mempty(qs\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ix, q \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(qs):\n\u001b[0;32m---> 24\u001b[0m     Cqs[ix] \u001b[38;5;241m=\u001b[39m \u001b[43mempirical_coverage\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred_d\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m qs, Cqs\n",
      "Cell \u001b[0;32mIn[88], line 15\u001b[0m, in \u001b[0;36mempirical_coverage\u001b[0;34m(y_true, y_pred_d, q)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mempirical_coverage\u001b[39m(y_true, y_pred_d, q):\n\u001b[0;32m---> 15\u001b[0m     low, up \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_intervals\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_pred_d\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m     comp \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlogical_and((y_true \u001b[38;5;241m<\u001b[39m up) , (y_true \u001b[38;5;241m>\u001b[39m low))\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mmean(comp)\n",
      "Cell \u001b[0;32mIn[88], line 7\u001b[0m, in \u001b[0;36mcompute_intervals\u001b[0;34m(predictions, q)\u001b[0m\n\u001b[1;32m      4\u001b[0m up \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mlen\u001b[39m(predictions))\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, pred \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(predictions):\n\u001b[0;32m----> 7\u001b[0m     sample \u001b[38;5;241m=\u001b[39m \u001b[43mpred\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m     low[i] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mquantile(sample, (\u001b[38;5;241m1.0\u001b[39m\u001b[38;5;241m-\u001b[39mq)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2.0\u001b[39m)\n\u001b[1;32m      9\u001b[0m     up[i]  \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mquantile(sample, \u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m-\u001b[39m (\u001b[38;5;241m1.0\u001b[39m\u001b[38;5;241m-\u001b[39mq)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2.0\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda/envs/tf-sbbo/lib/python3.8/site-packages/ngboost/distns/normal.py:76\u001b[0m, in \u001b[0;36mNormal.sample\u001b[0;34m(self, m)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msample\u001b[39m(\u001b[38;5;28mself\u001b[39m, m):\n\u001b[0;32m---> 76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrvs() \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(m)])\n",
      "File \u001b[0;32m~/miniconda/envs/tf-sbbo/lib/python3.8/site-packages/ngboost/distns/normal.py:76\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msample\u001b[39m(\u001b[38;5;28mself\u001b[39m, m):\n\u001b[0;32m---> 76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrvs\u001b[49m() \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(m)])\n",
      "File \u001b[0;32m~/miniconda/envs/tf-sbbo/lib/python3.8/site-packages/ngboost/distns/normal.py:81\u001b[0m, in \u001b[0;36mNormal.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattr__\u001b[39m(\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28mself\u001b[39m, name\n\u001b[1;32m     80\u001b[0m ):  \u001b[38;5;66;03m# gives us Normal.mean() required for RegressionDist.predict()\u001b[39;00m\n\u001b[0;32m---> 81\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mdir\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdist\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     82\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdist, name)\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "compute(y_test, y_pred_d, num_bins=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "19882a97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.33"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "empirical_coverage(y_test, y_pred_d, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "b3e43ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats\n",
    "\n",
    "low = np.zeros(len(y_pred_d))\n",
    "up = np.zeros(len(y_pred_d))\n",
    "q = 0.5\n",
    "\n",
    "for i in range(len(y_pred_d)): \n",
    "\n",
    "    m = y_pred_d[i].mean()\n",
    "    st = y_pred_d[i].std()\n",
    "\n",
    "    low[i] = scipy.stats.norm.ppf( (1 - q)/2, loc=m, scale=st)\n",
    "    up[i]  = scipy.stats.norm.ppf( 1 - (1 - q)/2, loc=m, scale=st)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "fd270669",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.22"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.logical_or(y_test < low, y_test > up))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "f5deaec1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.78"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.logical_and(y_test > low, y_test<up))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "f452d94d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.07610014,  1.40723512]), array([1.57532338, 2.53376406]))"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_intervals(y_pred_d[:2], 0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535e1b54",
   "metadata": {},
   "source": [
    "## BOCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ac0ea0da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempt Gibbs 1\n",
      "Iter Gibbs 0\n",
      "Iter Gibbs 100\n",
      "Iter Gibbs 200\n",
      "Iter Gibbs 300\n",
      "Iter Gibbs 400\n",
      "Iter Gibbs 500\n",
      "Iter Gibbs 600\n",
      "Iter Gibbs 700\n",
      "Iter Gibbs 800\n",
      "Iter Gibbs 900\n",
      "Iter Gibbs 1000\n",
      "Iter Gibbs 1100\n",
      "Iter Gibbs 1200\n",
      "Iter Gibbs 1300\n",
      "Iter Gibbs 1400\n",
      "Iter Gibbs 1500\n",
      "Iter Gibbs 1600\n",
      "Iter Gibbs 1700\n",
      "Iter Gibbs 1800\n",
      "Iter Gibbs 1900\n",
      "\n",
      "Test R^2: 0.364\n",
      "Test RMSE: 0.428 units\n",
      "Test MAE: 0.366 units\n"
     ]
    }
   ],
   "source": [
    "m2 = LinReg(nVars=25, order=2)\n",
    "m2.fit(X_train, y_train)\n",
    "y_pred = m2.predict(X_test)\n",
    "y_pred_d = m2.pred_dist(X_test)\n",
    "\n",
    "# Output R^2, RMSE and MAE on the test set\n",
    "score = r2_score(y_test, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "print(\"\\nTest R^2: {:.3f}\".format(score))\n",
    "print(\"Test RMSE: {:.3f} units\".format(rmse))\n",
    "print(\"Test MAE: {:.3f} units\".format(mae))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5f6c1704",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.        , 0.11111111, 0.22222222, 0.33333333, 0.44444444,\n",
       "        0.55555556, 0.66666667, 0.77777778, 0.88888889, 1.        ]),\n",
       " array([0. , 0.1, 0.2, 0.2, 0.4, 0.4, 0.6, 0.7, 0.9, 1. ]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scorer = CoveragePlot()\n",
    "scorer.compute(y_test, y_pred_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ddf322",
   "metadata": {},
   "source": [
    "## NGBoost - LinCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c04aad5e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iter 0] loss=0.7373 val_loss=0.0000 scale=1.0000 norm=0.6722\n",
      "[iter 100] loss=0.2726 val_loss=0.0000 scale=1.0000 norm=0.4678\n",
      "[iter 200] loss=-0.0798 val_loss=0.0000 scale=1.0000 norm=0.4156\n",
      "[iter 300] loss=-0.4203 val_loss=0.0000 scale=1.0000 norm=0.3880\n",
      "[iter 400] loss=-0.7433 val_loss=0.0000 scale=1.0000 norm=0.3668\n",
      "\n",
      "Test R^2: 0.674\n",
      "Test RMSE: 0.315 units\n",
      "Test MAE: 0.255 units\n"
     ]
    }
   ],
   "source": [
    "learner = LassoCV(cv=5)\n",
    "learner = DecisionTreeRegressor(criterion='friedman_mse', max_depth=5)  \n",
    "m3 = NGBRegressor(Base=learner)\n",
    "m3.fit(X_train, y_train)\n",
    "\n",
    "y_pred = m3.predict(X_test)\n",
    "\n",
    "# Output R^2, RMSE and MAE on the test set\n",
    "score = r2_score(y_test, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "print(\"\\nTest R^2: {:.3f}\".format(score))\n",
    "print(\"Test RMSE: {:.3f} units\".format(rmse))\n",
    "print(\"Test MAE: {:.3f} units\".format(mae))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f1824a8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.   0.   0.02 0.03 0.04 0.06 0.07 0.09 0.14 0.17 0.2  0.24 0.24 0.28\n",
      " 0.31 0.36 0.39 0.45 0.51 1.  ]\n"
     ]
    }
   ],
   "source": [
    "y_pred_d = m3.pred_dist(X_test)\n",
    "scorer = CVPPDiagram()\n",
    "qs, Cqs = scorer.compute(y_test, y_pred_d.mean(), y_pred_d.std(), num_bins=20)\n",
    "print(Cqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "58a92c27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.286359649122807"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scorer = AbsoluteMiscalibrationArea()\n",
    "scorer.compute(y_test, y_pred_d.mean(), y_pred_d.std(), num_bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "92e0ed19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.        , 0.11111111, 0.22222222, 0.33333333, 0.44444444,\n",
       "        0.55555556, 0.66666667, 0.77777778, 0.88888889, 1.        ]),\n",
       " array([0.  , 0.03, 0.04, 0.08, 0.16, 0.21, 0.26, 0.35, 0.43, 0.81]))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scorer = CoveragePlot()\n",
    "scorer.compute(y_test, y_pred_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4964f899",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.5141965434704472e-12"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_d = m3.pred_dist(X_test)\n",
    "\n",
    "y_pred_d[0].sample(10000).var()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07afefd",
   "metadata": {},
   "source": [
    "# BQP problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6adaa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 200\n",
    "opt = BQP(n=N)\n",
    "X = opt.X\n",
    "y = opt.y + 10.0\n",
    "test_set_size = 0.2\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_set_size, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d2f2aa",
   "metadata": {},
   "source": [
    "## NGBoost - LinCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a174a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "linCV = LassoCV(cv=5)\n",
    "lin = LinearRegression()\n",
    "dec = DecisionTreeRegressor(criterion='friedman_mse', max_depth=5)\n",
    "xgb = XGBRegressor()\n",
    "m3 = NGBRegressor(Base=lin)\n",
    "m3 = NGBRegressor(Base=xgb, learning_rate=0.1, verbose_eval=5, n_estimators=30, Dist=Normal, Score=LogScore)\n",
    "\n",
    "m3.fit(X_train, y_train)\n",
    "\n",
    "y_pred = m3.predict(X_test)\n",
    "\n",
    "# Output R^2, RMSE and MAE on the test set\n",
    "score = r2_score(y_test, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "print(\"\\nTest R^2: {:.3f}\".format(score))\n",
    "print(\"Test RMSE: {:.3f} units\".format(rmse))\n",
    "print(\"Test MAE: {:.3f} units\".format(mae))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd10544",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511b0f50",
   "metadata": {},
   "source": [
    "## GP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd472e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "m1 = GPr()\n",
    "m1.fit(X_train, y_train)\n",
    "y_pred = m1.predict(X_test)\n",
    "\n",
    "# Output R^2, RMSE and MAE on the test set\n",
    "score = r2_score(y_test, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "print(\"\\nTest R^2: {:.3f}\".format(score))\n",
    "print(\"Test RMSE: {:.3f} units\".format(rmse))\n",
    "print(\"Test MAE: {:.3f} units\".format(mae))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ced37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e51efab",
   "metadata": {},
   "source": [
    "## BOCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc43c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "m2 = LinReg(nVars=10, order=2,  nGibbs=500)\n",
    "m2.fit(X_train, y_train)\n",
    "y_pred = m2.predict(X_test)\n",
    "\n",
    "# Output R^2, RMSE and MAE on the test set\n",
    "score = r2_score(y_test, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "print(\"\\nTest R^2: {:.3f}\".format(score))\n",
    "print(\"Test RMSE: {:.3f} units\".format(rmse))\n",
    "print(\"Test MAE: {:.3f} units\".format(mae))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
